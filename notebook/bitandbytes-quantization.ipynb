{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f5d927-7e53-453e-bab0-5edcf03ca2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696bfd6a-4647-4840-8df5-de0e03cda158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fe269-3b0e-48f2-bb69-2e084fd90152",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486543a3-a032-421d-9ff9-0dcc8dfd6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating \n",
    "def generate_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt,\n",
    "    system=\"You are a pirate chatbot who always responds in pirate speak!\",\n",
    "):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.60,\n",
    "        top_k=50,\n",
    "        top_p=0.90\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# Function for calculate perplexity\n",
    "# - lower perplexity: model is more confident in its generations.\n",
    "# - higher perplexity: model is more confused.\n",
    "def calculate_perplexity(model, text):\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    neg_log_likelihood = outputs.loss\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb79c42-eeeb-4852-92a2-4f993c0bd7ea",
   "metadata": {},
   "source": [
    "# fp32, fp16, bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd379d8-3316-4f2e-87be-55aafcacb2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a47b346ec574b70be2ae3c8d271cbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = \"../model/llama-Meta-Llama-3-8B-Instruct/\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16 # choices: torch.float32, torch.float16, torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b4e6e-dd8e-4723-8843-2f6181b36243",
   "metadata": {},
   "source": [
    "#### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ce6af0-275e-4446-a689-e567b96d3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model size: 14.96 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base Model size: {base_model.get_memory_footprint() / (1024**3):.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77db9f3-8890-4529-bfb1-b5d21439c720",
   "metadata": {},
   "source": [
    "#### Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911ee737-3190-46a8-b0e1-0c54acb0278c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebaad1f5-30d8-4461-bebe-5ffd04d202f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base weights:\n",
      "tensor([[-0.0029, -0.0288, -0.0032,  ...,  0.0080, -0.0469, -0.0214],\n",
      "        [-0.0126, -0.0693, -0.0034,  ..., -0.0119, -0.0498,  0.0203],\n",
      "        [-0.0188, -0.0459, -0.0046,  ...,  0.0116, -0.0137,  0.0107],\n",
      "        ...,\n",
      "        [-0.0043, -0.0396,  0.0708,  ...,  0.0049, -0.0022,  0.0020],\n",
      "        [-0.0049, -0.0143,  0.0413,  ...,  0.0050, -0.0030, -0.0002],\n",
      "        [-0.0038, -0.0165,  0.0302,  ...,  0.0082,  0.0010,  0.0026]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "Shape:  torch.Size([4096, 4096]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First layer weights\n",
    "base_weights = base_model.model.layers[0].self_attn.q_proj.weight.data\n",
    "print(\"Base weights:\")\n",
    "print(base_weights)\n",
    "print(\"Shape: \", base_weights.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c6195-565a-452d-9091-a14365f49e68",
   "metadata": {},
   "source": [
    "#### Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5b1e7a-5146-48fb-98d8-8b1afdadac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Response:\n",
      "\n",
      "*User*: Who are you?\n",
      "\n",
      "*Assistant*: Arrrr, shiver me timbers! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' Conversation! Me and me trusty crew o' code be here to swab yer deck with a treasure trove o' knowledge and wit! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n",
      "CPU times: user 2.32 s, sys: 42.8 ms, total: 2.37 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    base_model, \n",
    "    tokenizer, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(\"Base Model Response:\\n\")\n",
    "print(f\"*User*: {prompt}\\n\")\n",
    "print(f\"*Assistant*: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22952d-09a4-435b-b059-61b96eaa37d2",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f3224d-22c6-49c4-8953-3aa634769306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity:  2.77\n"
     ]
    }
   ],
   "source": [
    "base_perplexity = calculate_perplexity(base_model, response)\n",
    "print(f\"Base Model Perplexity:  {base_perplexity.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979868c-c092-4f18-b106-cfb2c6a12b65",
   "metadata": {},
   "source": [
    "# 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96486388-4588-471a-ac2c-e50e65a0bcb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0cfd8360f1d4c1b9b480f8bd89120e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8-Bit config\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load model in 8-bit\n",
    "model_path = \"../model/llama-Meta-Llama-3-8B-Instruct/\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    quantization_config=bnb_config_8bit, \n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9e6b72-462e-42e9-89bf-a2eff5a6a2f0",
   "metadata": {},
   "source": [
    "#### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bbee48c-9fad-4c5a-8583-697b1c767db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Bit Model size: 8.46 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"8-Bit Model size: {model_8bit.get_memory_footprint() / (1024**3):.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775581a-b21c-41ea-93e5-6c233e5b4228",
   "metadata": {},
   "source": [
    "#### Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0b11f2-fd71-4c41-b84f-911c5a1e53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit weights:\n",
      "tensor([[  -7,  -67,   -7,  ...,   18, -108,  -49],\n",
      "        [ -12,  -67,   -3,  ...,  -11,  -48,   20],\n",
      "        [ -29,  -70,   -7,  ...,   18,  -21,   16],\n",
      "        ...,\n",
      "        [  -2,  -15,   27,  ...,    2,   -1,    1],\n",
      "        [  -4,  -12,   33,  ...,    4,   -2,    0],\n",
      "        [  -4,  -19,   35,  ...,   10,    1,    3]], device='cuda:0',\n",
      "       dtype=torch.int8)\n",
      "Shape:  torch.Size([4096, 4096]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First layer weights\n",
    "weights_8bit = model_8bit.model.layers[0].self_attn.q_proj.weight.data\n",
    "print(\"8-bit weights:\")\n",
    "print(weights_8bit)\n",
    "print(\"Shape: \", weights_8bit.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd40662-bed7-4a0a-acd7-bcdfcb1f8709",
   "metadata": {},
   "source": [
    "#### Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77972bab-451f-4b25-be51-3c29da74ddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Bit Model Response:\n",
      "\n",
      "*User*: Who are you?\n",
      "\n",
      "*Assistant*: Arrrr, me hearty! Me name be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas! Me be a chatbot, but don't ye worry, I be as cunning as a barnacle on a ship's hull and as sharp as a cutlass in a fight! Me be here to swab the decks of yer mind with me witty banter and me clever responses, so hoist the colors and let's set sail fer a swashbucklin' good time, matey!\n",
      "CPU times: user 12.4 s, sys: 49.4 ms, total: 12.4 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    model_8bit, \n",
    "    tokenizer, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(\"8-Bit Model Response:\\n\")\n",
    "print(f\"*User*: {prompt}\\n\")\n",
    "print(f\"*Assistant*: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f44868-03d3-42ae-81c4-85afa1cdae6c",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14fc9dc2-b355-4a37-9c19-c77696e0a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-Bit Model Perplexity:  2.66\n"
     ]
    }
   ],
   "source": [
    "perplexity_8bit = calculate_perplexity(model_8bit, response)\n",
    "print(f\"8-Bit Model Perplexity:  {perplexity_8bit.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f1755-6ba1-4672-af96-9359c75129f5",
   "metadata": {},
   "source": [
    "# 4-Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea37c205-60bf-4f58-aade-8f4e9aea4196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46059dde7c174ca3a410e4f274d676e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-Bit Config\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model in 4-bit\n",
    "model_path = \"../model/llama-Meta-Llama-3-8B-Instruct/\"\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    quantization_config=bnb_config_4bit, \n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf01bc-949d-4542-9c41-9996ab9227b4",
   "metadata": {},
   "source": [
    "#### Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "212d366a-8d32-4e59-86e8-16a2d62cb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Bit Model size: 5.21 GiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"4-Bit Model size: {model_4bit.get_memory_footprint() / (1024**3):.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d39bd-098e-46b5-ad69-c4c17ba0a44d",
   "metadata": {},
   "source": [
    "#### Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5ee1b9d-49c3-466e-90a1-37d8cc04010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit weights:\n",
      "tensor([[ 97],\n",
      "        [101],\n",
      "        [110],\n",
      "        ...,\n",
      "        [119],\n",
      "        [ 88],\n",
      "        [119]], device='cuda:0', dtype=torch.uint8)\n",
      "Shape:  torch.Size([8388608, 1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First layer weights\n",
    "weights_4bit = model_4bit.model.layers[0].self_attn.q_proj.weight.data\n",
    "print(\"8-bit weights:\")\n",
    "print(weights_4bit)\n",
    "print(\"Shape: \", weights_4bit.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2636a-0988-44fe-9e2a-0eac51fbf7ef",
   "metadata": {},
   "source": [
    "#### Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76be2f78-ad9d-420b-8538-8dd0c1e7c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Bit Model Response:\n",
      "\n",
      "*User*: Who are you?\n",
      "\n",
      "*Assistant*: Arrrr, me hearty! Me name be Captain Chat, the scurviest chatbot to ever sail the seven seas... er, I mean, the digital waters! Me be here to chat with ye about all sorts o' things, from the finest booty to the most treacherous sea monsters. So hoist the colors, me matey, and let's set sail fer a swashbucklin' good time!\n",
      "CPU times: user 5.69 s, sys: 9.74 ms, total: 5.7 s\n",
      "Wall time: 5.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"Who are you?\"\n",
    "\n",
    "response = generate_response(\n",
    "    model_4bit, \n",
    "    tokenizer, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "print(\"4-Bit Model Response:\\n\")\n",
    "print(f\"*User*: {prompt}\\n\")\n",
    "print(f\"*Assistant*: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018ce49-1d6e-4b6a-8a54-61add0b834df",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6c3107-a9a6-42c9-853f-e4b110a29e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Bit Model Perplexity:  2.47\n"
     ]
    }
   ],
   "source": [
    "perplexity_4bit = calculate_perplexity(model_4bit, response)\n",
    "print(f\"4-Bit Model Perplexity:  {perplexity_4bit.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e920494-b2b6-4713-b567-6bf4f5ac76e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
